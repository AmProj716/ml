# Regularisation


Regularisation L1 & L2

from sklearn.linear_model import Ridge, Lasso

#lass0
lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X, y)
y_pred_lasso = lasso_model.predict(X)

#ridge
ridge_model = Ridge(alpha=0.1)
ridge_model.fit(X, y)
y_pred_ridge = ridge_model.predict(X)

#test
mse_lasso = mean_squared_error(y, y_pred_lasso)
r2_lasso = r2_score(y, y_pred_lasso)

mse_ridge = mean_squared_error(y, y_pred_ridge)
r2_ridge = r2_score(y, y_pred_ridge)

print("\nL1 Regularization (Lasso) - Accuracy:")
print("MSE:", mse_lasso, " R2:", r2_lasso)

print("\nL2 Regularization (Ridge) - Accuracy:")
print("MSE:", mse_ridge, " R2:", r2_ridge)


import numpy as np

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()
X[:, 1:] = scaler.fit_transform(X[:, 1:])

learning_rate = 0.0001
num_iterations = 10000
beta_l1 = np.zeros(X.shape[1])
beta_l2 = np.zeros(X.shape[1])
l1_penalty = 0.1
l2_penalty = 0.1





X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)


X = replace_nan_with_mean(X)
y = np.nan_to_num(y)

learning_rate = 0.001
num_iterations = 10000
beta_l1 = np.zeros(X.shape[1])
beta_l2 = np.zeros(X.shape[1])
l1_penalty = 0.1
l2_penalty = 0.1

for _ in range(num_iterations):
    y_pred_l1 = X @ beta_l1
    error_l1 = y - y_pred_l1

    # Calculate gradient with L1 regularization term
    l1_gradient = l1_penalty * np.sign(beta_l1)
    gradient_l1 = -2 * X.T @ error_l1 / X.shape[0] + l1_gradient

    beta_l1 -= learning_rate * gradient_l1

# Gradient Descent with L2 Regularization
for _ in range(num_iterations):
    y_pred_l2 = X @ beta_l2
    error_l2 = y - y_pred_l2


    l2_gradient = 2 * l2_penalty * beta_l2
    gradient_l2 = -2 * X.T @ error_l2 / X.shape[0] + l2_gradient

    beta_l2 -= learning_rate * gradient_l2

#
y_pred_l1 = X @ beta_l1
y_pred_l2 = X @ beta_l2

# mse
mse_l1 = np.mean((y - y_pred_l1) ** 2)
r2_l1 = 1 - (np.sum((y - y_pred_l1) ** 2) / np.sum((y - np.mean(y)) ** 2))

mse_l2 = np.mean((y - y_pred_l2) ** 2)
r2_l2 = 1 - (np.sum((y - y_pred_l2) ** 2) / np.sum((y - np.mean(y)) ** 2))

print("\nMultiple Linear Regression with L1 Regularization - Accuracy:")
print("MSE:", mse_l1, " R2:", r2_l1)

print("\nMultiple Linear Regression with L2 Regularization - Accuracy:")
print("MSE:", mse_l2, " R2:", r2_l2)

print("\nCoefficients with L1 Regularization:")
for i in range(len(beta_l1)):
    print(f"Coefficient {i}:", beta_l1[i])

print("\nCoefficients with L2 Regularization:")
for i in range(len(beta_l2)):
    print(f"Coefficient {i}:", beta_l2[i])


regularisation without library

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
df = pd.read_csv("/content/drive/MyDrive/resourcesforpr/50_Startups.csv")
x = df.iloc[:,:-1].values
y = df.iloc[:,-1].values
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
x = np.array(ct.fit_transform(x))
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x = sc.fit_transform(x)

Xtrain, Xtest, Ytrain, Ytest = train_test_split( x, y, test_size = 0.3,random_state=42)
def CostFunction(x,y,w,b):
    cost = np.sum((((x.dot(w) + b) - y) ** 2) / (2*len(y)))
    return cost

def GradientDescent(x, y, w, b, learning_rate, epochs):
    cost_list = [0] * epochs

    for epoch in range(epochs):
        z = x.dot(w) + b
        loss = z - y

        weight_gradient = x.T.dot(loss) / len(y)
        bias_gradient = np.sum(loss) / len(y)

        w = w - learning_rate*weight_gradient
        b = b - learning_rate*bias_gradient

        cost = CostFunction(x, y, w, b)
        cost_list[epoch] = cost

        if (epoch%(epochs/10)==0):
            print("Cost is:",cost)

    return w, b, cost_list
w, b, c= GradientDescent(Xtrain, Ytrain, np.zeros(Xtrain.shape[1]), 0, 0.002,epochs=15000)
plt.plot(c)
print('hi')

def predict(X, w, b):
    return X.dot(w) + b
y_pred = predict(Xtest, w, b)

def r2score(y_pred, y):
    rss = np.sum((y_pred - y) ** 2)
    tss = np.sum((y-y.mean()) ** 2)

    r2 = 1 - (rss / tss)
    return r2
r2score(y_pred, Ytest)
