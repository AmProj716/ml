# California housing (linear fitting)

#!pip install scikit-learn

from sklearn import linear_model
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing

housing=fetch_california_housing()
print(housing.data.shape, housing.target.shape)

df=pd.DataFrame(housing.data, columns=housing.feature_names)
df['target']=housing.target
df.head()






import altair as alt
alt.data_transformers.disable_max_rows()

# Univariate distributions
for column in df.columns:
    if df[column].dtype in ['number', 'float']:
        chart = alt.Chart(df).mark_bar().encode(
            alt.X(column, bin=True),
            alt.Y('count()')
        ).properties(
            title=f"Distribution of {column}"
        )
        display(chart)

# Bivariate relationships
for column1 in df.columns:
    for column2 in df.columns:
        if df[column1].dtype in ['number', 'float'] and df[column2].dtype in ['number', 'float'] and column1 != column2:
            chart = alt.Chart(df).mark_circle().encode(
                alt.X(column1),
                alt.Y(column2),
                color='target'
            ).properties(
                title=f"Relationship between {column1} and {column2}"
            )
            display(chart)


from google.colab import sheets
sheet = sheets.InteractiveSheet(df=df)



df.isnull().sum()


#NORMAL EQUATION WITHOUT SKLEARN
import matplotlib.pyplot as plt
import numpy as np

x = df['MedInc'].values
y = df['target'].values


x_mean = np.mean(x)
y_mean = np.mean(y)


numerator = np.sum((x - x_mean) * (y - y_mean))
denominator = np.sum((x - x_mean) ** 2)
b1 = numerator / denominator
b0 = y_mean - b1 * x_mean


y_pred_simple = b0 + b1 * x


plt.scatter(x, y, label='Data')
plt.plot(x, y_pred_simple, color='red', label='Simple Regression Line')
plt.xlabel('Median Income')
plt.ylabel('Target')
plt.title('Simple Linear Regression')
plt.legend()
plt.show()
from mpl_toolkits.mplot3d import Axes3D

# Extract the two features used in multiple regression
X1 = df['MedInc'].values
X2 = df['HouseAge'].values
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X1, X2, y, c='blue', marker='o', label='Data')

x1_range = np.linspace(X1.min(), X1.max(), 50)
x2_range = np.linspace(X2.min(), X2.max(), 50)
X1_mesh, X2_mesh = np.meshgrid(x1_range, x2_range)
X_mesh = np.concatenate((np.ones((X1_mesh.size, 1)), X1_mesh.ravel().reshape(-1, 1), X2_mesh.ravel().reshape(-1, 1)), axis=1)
y_pred_mesh = X_mesh @ beta_hat
Y_mesh = y_pred_mesh.reshape(X1_mesh.shape)
ax.plot_surface(X1_mesh, X2_mesh, Y_mesh, color='red', alpha=0.5, label='Regression Plane')
ax.set_xlabel('Median Income')
ax.set_ylabel('House Age')
ax.set_zlabel('Target')
ax.set_title('Multiple Linear Regression - 3D Visualization')
plt.show()


X = df[['MedInc', 'HouseAge']].values
y = df['target'].values


X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)

beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y


y_pred_multiple = X @ beta_hat



plt.scatter(y, y_pred_multiple)
plt.xlabel('Actual Target')
plt.ylabel('Predicted Target')
plt.title('Multiple Linear Regression: Actual vs Predicted')
plt.show()

print("Simple Linear Regression Coefficients:")
print("Intercept (b0):", b0)
print("Slope (b1):", b1)

print("\nMultiple Linear Regression Coefficients:")
print("Intercept:", beta_hat[0])
print("Coefficient for median_income:", beta_hat[1])
print("Coefficient for housing_median_age:", beta_hat[2])


#GRADIENT DECENT WITHOUT SKLEARN

import matplotlib.pyplot as plt
import numpy as np


# Initialize parameters
learning_rate = 0.01
num_iterations = 1000
b0 = 0
b1 = 0

# Gradient Descent
for _ in range(num_iterations):
  y_pred = b0 + b1 * x
  error = y - y_pred
  b0_gradient = -2 * np.mean(error)
  b1_gradient = -2 * np.mean(x * error)
  b0 -= learning_rate * b0_gradient
  b1 -= learning_rate * b1_gradient

# Predictions
y_pred_gd_simple = b0 + b1 * x

# Plot
plt.scatter(x, y, label='Data')
plt.plot(x, y_pred_gd_simple, color='green', label='Simple Regression Line (GD)')
plt.xlabel('Median Income')
plt.ylabel('Target')
plt.title('Simple Linear Regression with Gradient Descent')
plt.legend()
plt.show()

# Multiple Linear Regression with Gradient Descent

learning_rate = 0.001
num_iterations = 10000
beta = np.zeros(X.shape[1])

# Gradient Descent
for _ in range(num_iterations):
  y_pred = X @ beta
  error = y - y_pred
  gradient = -2 * X.T @ error / X.shape[0]
  beta -= learning_rate * gradient

# Predictions
y_pred_gd_multiple = X @ beta


plt.scatter(y, y_pred_gd_multiple)
plt.xlabel('Actual Target')
plt.ylabel('Predicted Target (GD)')
plt.title('Multiple Linear Regression with Gradient Descent: Actual vs Predicted')
plt.show()

print("Simple Linear Regression Coefficients (Gradient Descent):")
print("Intercept (b0):", b0)
print("Slope (b1):", b1)

print("\nMultiple Linear Regression Coefficients (Gradient Descent):")
for i in range(len(beta)):
  print(f"Coefficient {i}:", beta[i])


# Simple Linear Regression
# Normal Equation
mse_simple_normal = mean_squared_error(y, y_pred_simple)
r2_simple_normal = r2_score(y, y_pred_simple)

# Gradient Descent
mse_simple_gd = mean_squared_error(y, y_pred_gd_simple)
r2_simple_gd = r2_score(y, y_pred_gd_simple)

print("\nSimple Linear Regression - Accuracy Comparison:")
print("Normal Equation - MSE:", mse_simple_normal, " R2:", r2_simple_normal)
print("Gradient Descent - MSE:", mse_simple_gd, " R2:", r2_simple_gd)

# Multiple Linear Regression
# Normal Equation
mse_multiple_normal = mean_squared_error(y, y_pred_multiple)
r2_multiple_normal = r2_score(y, y_pred_multiple)

# gradient Descent
mse_multiple_gd = mean_squared_error(y, y_pred_gd_multiple)
r2_multiple_gd = r2_score(y, y_pred_gd_multiple)

print("\nMultiple Linear Regression - Accuracy Comparison:")
print("Normal Equation - MSE:", mse_multiple_normal, " R2:", r2_multiple_normal)
print("Gradient Descent - MSE:", mse_multiple_gd, " R2:", r2_multiple_gd)






# Simple Linear Regression using sk learn
X_simple = df['MedInc'].values.reshape(-1, 1)
y_simple = df['target'].values

model_simple = LinearRegression()
model_simple.fit(X_simple, y_simple)

y_pred_sklearn_simple = model_simple.predict(X_simple)

X_multiple = df[['MedInc', 'HouseAge']].values
y_multiple = df['target'].values

model_multiple = LinearRegression()
model_multiple.fit(X_multiple, y_multiple)

y_pred_sklearn_multiple = model_multiple.predict(X_multiple)
mse_sklearn_simple = mean_squared_error(y_simple, y_pred_sklearn_simple)
r2_sklearn_simple = r2_score(y_simple, y_pred_sklearn_simple)

print("\nSklearn Simple Linear Regression - Accuracy:")
print("MSE:", mse_sklearn_simple, " R2:", r2_sklearn_simple)

mse_sklearn_multiple = mean_squared_error(y_multiple, y_pred_sklearn_multiple)
r2_sklearn_multiple = r2_score(y_multiple, y_pred_sklearn_multiple)

print("\nSklearn Multiple Linear Regression - Accuracy:")
print("MSE:", mse_sklearn_multiple, " R2:", r2_sklearn_multiple)
